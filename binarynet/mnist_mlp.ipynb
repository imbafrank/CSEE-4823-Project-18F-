{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n",
      "Can not use cuDNN on context None: cannot compile with cuDNN. We got this error:\n",
      "/usr/bin/ld: cannot find -lcudnn\n",
      "collect2: ld returned 1 exit status\n",
      "\n",
      "Preallocating 4875/12189 Mb (0.400000) on cuda0\n",
      "Mapped name None to device cuda0: TITAN X (Pascal) (0000:02:00.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000 train samples\n",
      "10000 test samples\n"
     ]
    }
   ],
   "source": [
    "'''Trains a simple binarize fully connected NN on the MNIST dataset.\n",
    "Modified from keras' examples/mnist_mlp.py\n",
    "Gets to 97.9% test accuracy after 20 epochs using theano backend\n",
    "'''\n",
    "\n",
    "\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "np.random.seed(1337)  # for reproducibility\n",
    "\n",
    "import keras.backend as K\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, BatchNormalization\n",
    "from keras.optimizers import SGD, Adam, RMSprop\n",
    "from keras.callbacks import LearningRateScheduler\n",
    "from keras.utils import np_utils\n",
    "\n",
    "from binary_ops import binary_tanh as binary_tanh_op\n",
    "from binary_layers import BinaryDense, Clip\n",
    "\n",
    "from keras.models import load_model\n",
    "\n",
    "\n",
    "class DropoutNoScale(Dropout):\n",
    "    '''Keras Dropout does scale the input in training phase, which is undesirable here.\n",
    "    '''\n",
    "    def call(self, inputs, training=None):\n",
    "        if 0. < self.rate < 1.:\n",
    "            noise_shape = self._get_noise_shape(inputs)\n",
    "\n",
    "            def dropped_inputs():\n",
    "                return K.dropout(inputs, self.rate, noise_shape,\n",
    "                                 seed=self.seed) * (1 - self.rate)\n",
    "            return K.in_train_phase(dropped_inputs, inputs,\n",
    "                                    training=training)\n",
    "        return inputs\n",
    "\n",
    "def binary_tanh(x):\n",
    "    return binary_tanh_op(x)\n",
    "\n",
    "\n",
    "batch_size = 100\n",
    "epochs = 20\n",
    "nb_classes = 10\n",
    "input_num = 784\n",
    "\n",
    "H = 'Glorot'\n",
    "kernel_lr_multiplier = 'Glorot'\n",
    "\n",
    "# network\n",
    "num_unit = 1024\n",
    "num_hidden = 3\n",
    "use_bias = False\n",
    "\n",
    "# learning rate schedule\n",
    "lr_start = 1e-3\n",
    "lr_end = 1e-4\n",
    "lr_decay = (lr_end / lr_start)**(1. / epochs)\n",
    "\n",
    "# BN\n",
    "epsilon = 1e-4\n",
    "momentum = 0.9\n",
    "\n",
    "# dropout\n",
    "drop_in = 0.2\n",
    "drop_hidden = 0.5\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 784)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.bool_'>\n"
     ]
    }
   ],
   "source": [
    "print(X_test.shape)\n",
    "X_test = X_test>0\n",
    "print(type(X_test))\n",
    "print(type(X_test[0,0]))\n",
    "X_test = X_test.astype(int)\n",
    "for i in range(X_test.shape[0]):\n",
    "    for j in range(X_test.shape[1]):\n",
    "        if X_test[i,j] == 0:\n",
    "            X_test[i,j] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "<type 'numpy.ndarray'>\n",
      "<type 'numpy.bool_'>\n"
     ]
    }
   ],
   "source": [
    "print(X_train.shape)\n",
    "X_train = X_train>0\n",
    "print(type(X_train))\n",
    "print(type(X_train[0,0]))\n",
    "X_train = X_train.astype(int)\n",
    "for i in range(X_train.shape[0]):\n",
    "    for j in range(X_train.shape[1]):\n",
    "        if X_train[i,j] == 0:\n",
    "            X_train[i,j] = -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense0 (BinaryDense)         (None, 1024)              802816    \n",
      "_________________________________________________________________\n",
      "act0 (Activation)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense1 (BinaryDense)         (None, 1024)              1048576   \n",
      "_________________________________________________________________\n",
      "act1 (Activation)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense2 (BinaryDense)         (None, 1024)              1048576   \n",
      "_________________________________________________________________\n",
      "act2 (Activation)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense3 (BinaryDense)         (None, 1024)              1048576   \n",
      "_________________________________________________________________\n",
      "act3 (Activation)            (None, 1024)              0         \n",
      "_________________________________________________________________\n",
      "dense (BinaryDense)          (None, 10)                10240     \n",
      "=================================================================\n",
      "Total params: 3,958,784\n",
      "Trainable params: 3,958,784\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# convert class vectors to binary class matrices\n",
    "Y_train = np_utils.to_categorical(y_train, nb_classes) * 2 - 1 # -1 or 1 for hinge loss\n",
    "Y_test = np_utils.to_categorical(y_test, nb_classes) * 2 - 1\n",
    "\n",
    "model = Sequential()\n",
    "# model.add(DropoutNoScale(drop_in, input_shape=(784,), name='drop0'))\n",
    "model.add(BinaryDense(num_unit, input_shape=(input_num,), use_bias=use_bias, name=\"dense0\"))\n",
    "model.add(Activation(binary_tanh, name='act0'))\n",
    "for i in range(num_hidden):\n",
    "    model.add(BinaryDense(num_unit, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias,\n",
    "              name='dense{}'.format(i+1)))\n",
    "#     model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn{}'.format(i+1)))\n",
    "    model.add(Activation(binary_tanh, name='act{}'.format(i+1)))\n",
    "#     model.add(DropoutNoScale(drop_hidden, name='drop{}'.format(i+1)))\n",
    "model.add(BinaryDense(10, H=H, kernel_lr_multiplier=kernel_lr_multiplier, use_bias=use_bias,\n",
    "          name='dense'))\n",
    "\n",
    "# model.add(BatchNormalization(epsilon=epsilon, momentum=momentum, name='bn'))\n",
    "# model.add(Activation(binary_tanh, name='act'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.1508 - acc: 0.8648 - val_loss: 0.0849 - val_acc: 0.9143\n",
      "Epoch 2/20\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0781 - acc: 0.9228 - val_loss: 0.0663 - val_acc: 0.9328\n",
      "Epoch 3/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0764 - acc: 0.9304 - val_loss: 0.0645 - val_acc: 0.9388\n",
      "Epoch 4/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0672 - acc: 0.9397 - val_loss: 0.0652 - val_acc: 0.9457\n",
      "Epoch 5/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0597 - acc: 0.9473 - val_loss: 0.1023 - val_acc: 0.9145\n",
      "Epoch 6/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0523 - acc: 0.9533 - val_loss: 0.0674 - val_acc: 0.9485\n",
      "Epoch 7/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0490 - acc: 0.9563 - val_loss: 0.0714 - val_acc: 0.9464\n",
      "Epoch 8/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0461 - acc: 0.9598 - val_loss: 0.0600 - val_acc: 0.9498\n",
      "Epoch 9/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0373 - acc: 0.9669 - val_loss: 0.0676 - val_acc: 0.9527\n",
      "Epoch 10/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0363 - acc: 0.9690 - val_loss: 0.0571 - val_acc: 0.9565\n",
      "Epoch 11/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0287 - acc: 0.9740 - val_loss: 0.0618 - val_acc: 0.9563\n",
      "Epoch 12/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0259 - acc: 0.9769 - val_loss: 0.0588 - val_acc: 0.9568\n",
      "Epoch 13/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0197 - acc: 0.9816 - val_loss: 0.0590 - val_acc: 0.9565\n",
      "Epoch 14/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0193 - acc: 0.9823 - val_loss: 0.0677 - val_acc: 0.9572\n",
      "Epoch 15/20\n",
      "60000/60000 [==============================] - 2s 25us/step - loss: 0.0149 - acc: 0.9856 - val_loss: 0.0607 - val_acc: 0.9613\n",
      "Epoch 16/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0125 - acc: 0.9878 - val_loss: 0.0576 - val_acc: 0.9624\n",
      "Epoch 17/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0106 - acc: 0.9895 - val_loss: 0.0491 - val_acc: 0.9673\n",
      "Epoch 18/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0086 - acc: 0.9916 - val_loss: 0.0530 - val_acc: 0.9678\n",
      "Epoch 19/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0082 - acc: 0.9922 - val_loss: 0.0555 - val_acc: 0.9678\n",
      "Epoch 20/20\n",
      "60000/60000 [==============================] - 2s 26us/step - loss: 0.0056 - acc: 0.9942 - val_loss: 0.0545 - val_acc: 0.9657\n",
      "Test score: 0.054517130958503\n",
      "Test accuracy: 0.9657\n"
     ]
    }
   ],
   "source": [
    "opt = Adam(lr=lr_start) \n",
    "model.compile(loss='squared_hinge', optimizer=opt, metrics=['acc'])\n",
    "\n",
    "# deserialized custom layers\n",
    "#model.save('mlp.h5')\n",
    "#model = load_model('mlp.h5', custom_objects={'DropoutNoScale': DropoutNoScale,\n",
    "#                                             'BinaryDense': BinaryDense,\n",
    "#                                             'Clip': Clip, \n",
    "#                                             'binary_tanh': binary_tanh})\n",
    "\n",
    "lr_scheduler = LearningRateScheduler(lambda e: lr_start * lr_decay ** e)\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size, epochs=epochs,\n",
    "                    verbose=1, validation_data=(X_test, Y_test),\n",
    "                    callbacks=[lr_scheduler])\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer_list = ['dense0','dense1','dense2','dense3','dense']\n",
    "kernel_list = []\n",
    "for layer_id in layer_list:\n",
    "    layer = model.get_layer(name=layer_id)\n",
    "    kernel = layer.get_kernel().eval()\n",
    "    kernel_list.append(kernel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784, 1024)\n",
      "(1024, 1024)\n",
      "(1024, 1024)\n",
      "(1024, 1024)\n",
      "(1024, 10)\n"
     ]
    }
   ],
   "source": [
    "for kernel in kernel_list:\n",
    "    print(kernel.shape)\n",
    "import pickle\n",
    "with open('kernel_matrix_3hidden_1024_9657.pkl', 'wb') as f:\n",
    "    pickle.dump(kernel_list, f)\n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
